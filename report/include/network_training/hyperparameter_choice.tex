Having an inexhaustible source of annotated images, the selected models can be
trained and evaluated, and a comparison of their performances can be drawn.

\subsection{Choice of hyper-parameters}

To begin with, the most tedious and time-consuming task was to choose the right
hyper-parameters for the most efficient training, and to obtain the best
results in the shortest time.

The complete implementation is written in Python using the Keras wrapper for
TensorFlow, and runs on the GPU for accelerated performance. The dataset,
stored on an SSD for optimal I/O throughput, is loaded in batches, and the
images are augmented on the fly. Using a standard batch size of 32, as suggested
by Loquercio~\etal~\cite{dronet}, the training data is in a different order at
each iteration, and the image augmentations are randomized and applied on
different base images at each epoch, thus increasing the amount of samples seen
by the network. The image augmentation process is discussed more thoroughly in
a chapter 4.\\

Concerning the optimization algorithm, Adam~\cite{Adam}, its hyper-parameters
such as the learning rate and the decay factor were left to their default
values. By using a relatively small subset of the training data, it is much
easier to experiment with the learning rate and the decay factor with several
short training phases.

A recurrent issue that was observed was the loss converging too early during
the training, needing to damper the learning rate every few iterations. To
automate this, the convenient callback offered by Keras,
\pyth{ReduceLROnPlateau}, is added to the optimization process.

\begin{figure}[h]
    \centering
    \caption{Keras' convenient ReduceLROnPlateau callback function.}
    \begin{python}
        reduce_lr = ReduceLROnPlateau(monitor="val_loss",
            factor=0.9, patience=5, verbose=1)
    \end{python}
\end{figure}

Its role is to detect the presence of a ``plateau'' in the learning curve, by
using a given threshold for the validation loss difference over a certain
amount of iterations (the patience argument), and to reduce the learning rate
by a certain factor in consequence.\\

Concerning MobileNetV2, the $\alpha$ parameter mentioned previously, responsible
for setting the size of the depthwise convolution kernels, is set to $0.35$ for
the entirety of the experiment. This choice is justified by basic trial and
error of possible values $0.35$, $0.5$ and $1$, which clearly showed that
values above $0.35$ contributed to overfit the model on synthetic images.
Effectively, running each trained model on a manually recorded test dataset of
real gates clearly showed that the network was unable to detect gates for the
higher $\alpha$ values, even if the validation accuracy was higher.

This can intuitively be explained by the feature extraction being too precise
to generalize the shape of a gate, and as a result, the network could
discriminate real gates from its perfect knowledge of fake gates.\\

Lastly, in order to make it easier for the network to approximate an objective
function for the given input data, the latter is normalized into the $[0.0,1.0]$
range.
